#!/usr/bin/env python3
"""
æ‰¹é‡è·å–arXiv CSé¢†åŸŸæœ€è¿‘6ä¸ªæœˆæ‰€æœ‰è®ºæ–‡çš„metadataå¹¶ä¿å­˜ä¸ºparquetæ ¼å¼

ä½¿ç”¨OAI-PMHåè®®é«˜æ•ˆè·å–å¤§é‡å…ƒæ•°æ®ï¼Œé¿å…APIé¢‘ç‡é™åˆ¶
æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¢é‡æ›´æ–°
"""

import os
import sys
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
import xml.etree.ElementTree as ET
import pandas as pd
import requests
from urllib.parse import urlencode

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bulk_cs_metadata.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# CSåˆ†ç±»åˆ—è¡¨ - åŒ…å«æ‰€æœ‰CSå­é¢†åŸŸ
CS_CATEGORIES = {
    'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG', 'cs.CL', 'cs.CR', 'cs.CV', 
    'cs.CY', 'cs.DB', 'cs.DC', 'cs.DL', 'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL',
    'cs.GL', 'cs.GR', 'cs.GT', 'cs.HC', 'cs.IR', 'cs.IT', 'cs.LG', 'cs.LO',
    'cs.MA', 'cs.MM', 'cs.MS', 'cs.NA', 'cs.NE', 'cs.NI', 'cs.OH', 'cs.OS',
    'cs.PF', 'cs.PL', 'cs.RO', 'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY'
}

# OAI-PMHå‘½åç©ºé—´
NAMESPACES = {
    'oai': 'http://www.openarchives.org/OAI/2.0/',
    'oai_dc': 'http://www.openarchives.org/OAI/2.0/oai_dc/',
    'dc': 'http://purl.org/dc/elements/1.1/',
    'arXiv': 'http://arxiv.org/OAI/arXiv/'
}

class ArxivBulkFetcher:
    """arXivæ‰¹é‡å…ƒæ•°æ®è·å–å™¨"""
    
    def __init__(self, output_dir: str = "arxiv_data", months_back: int = 6):
        self.base_url = "http://export.arxiv.org/oai2"
        self.output_dir = output_dir
        self.months_back = months_back
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArxivBulkFetcher/1.0 (Research Tool)'
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ç”¨äºè·Ÿè¸ªå¤„ç†è¿›åº¦çš„æ–‡ä»¶
        self.progress_file = os.path.join(self.output_dir, "progress.txt")
        self.checkpoint_file = os.path.join(self.output_dir, "checkpoint.txt")
        
    def get_date_range(self) -> tuple:
        """è·å–æ—¥æœŸèŒƒå›´"""
        # arXivæ•°æ®é€šå¸¸æœ‰1-2å¤©å»¶è¿Ÿï¼Œæ‰€ä»¥ç»“æŸæ—¥æœŸè®¾ä¸ºæ˜¨å¤©
        end_date = datetime.now() - timedelta(days=1)
        start_date = end_date - timedelta(days=self.months_back * 30)
        
        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    
    def load_checkpoint(self) -> Optional[str]:
        """åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, 'r') as f:
                return f.read().strip()
        return None
    
    def save_checkpoint(self, resumption_token: str):
        """ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        with open(self.checkpoint_file, 'w') as f:
            f.write(resumption_token)
    
    def clear_checkpoint(self):
        """æ¸…é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯"""
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)
    
    def make_oai_request(self, verb: str, params: Dict[str, str]) -> ET.Element:
        """å‘èµ·OAI-PMHè¯·æ±‚"""
        params['verb'] = verb
        url = f"{self.base_url}?{urlencode(params)}"
        
        logger.debug(f"OAI request: {url}")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # è§£æXMLå“åº”
                root = ET.fromstring(response.content)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                error = root.find('.//oai:error', NAMESPACES)
                if error is not None:
                    error_code = error.get('code', 'unknown')
                    error_msg = error.text or 'Unknown error'
                    raise Exception(f"OAI error {error_code}: {error_msg}")
                
                return root
                
            except Exception as e:
                logger.warning(f"Request failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    raise
    
    def extract_metadata_from_record(self, record: ET.Element) -> Optional[Dict]:
        """ä»OAIè®°å½•ä¸­æå–å…ƒæ•°æ®"""
        try:
            header = record.find('oai:header', NAMESPACES)
            if header is None:
                return None
            
            # æ£€æŸ¥æ˜¯å¦è¢«åˆ é™¤
            if header.get('status') == 'deleted':
                return None
                
            # è·å–åŸºæœ¬ä¿¡æ¯
            identifier = header.find('oai:identifier', NAMESPACES)
            datestamp = header.find('oai:datestamp', NAMESPACES)
            
            if identifier is None or datestamp is None:
                return None
            
            arxiv_id = identifier.text.replace('oai:arXiv.org:', '')
            
            # è·å–åˆ†ç±»ä¿¡æ¯
            set_specs = header.findall('oai:setSpec', NAMESPACES)
            categories = [spec.text for spec in set_specs if spec.text]
            
            # è¿‡æ»¤CSåˆ†ç±»
            cs_categories = [cat for cat in categories if cat.startswith('cs')]
            if not cs_categories:
                return None
            
            # è·å–è¯¦ç»†å…ƒæ•°æ®
            metadata = record.find('.//oai_dc:dc', NAMESPACES)
            if metadata is None:
                return None
            
            # æå–å„å­—æ®µ
            def get_text_list(element_name):
                elements = metadata.findall(f'dc:{element_name}', NAMESPACES)
                return [elem.text for elem in elements if elem.text]
            
            def get_text_first(element_name):
                elements = get_text_list(element_name)
                return elements[0] if elements else None
            
            paper_data = {
                'arxiv_id': arxiv_id,
                'title': get_text_first('title'),
                'authors': '; '.join(get_text_list('creator')),
                'abstract': get_text_first('description'),
                'subject_categories': '; '.join(categories),
                'cs_categories': '; '.join(cs_categories),
                'date_submitted': datestamp.text,
                'last_updated': datestamp.text,
                'doi': get_text_first('relation'),
                'journal_ref': get_text_first('source'),
                'fetch_time': datetime.now().isoformat()
            }
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not paper_data['title'] or not paper_data['authors']:
                logger.debug(f"Skipping paper {arxiv_id} due to missing title/authors")
                return None
                
            return paper_data
            
        except Exception as e:
            logger.warning(f"Error extracting metadata from record: {e}")
            return None
    
    def fetch_cs_papers_batch(self, resumption_token: Optional[str] = None) -> List[Dict]:
        """æ‰¹é‡è·å–CSè®ºæ–‡å…ƒæ•°æ®"""
        papers = []
        
        if resumption_token:
            params = {'resumptionToken': resumption_token}
            logger.info(f"Resuming from token: {resumption_token[:50]}...")
        else:
            # è®¾ç½®æ—¥æœŸèŒƒå›´
            start_date, end_date = self.get_date_range()
            logger.info(f"Fetching CS papers from {start_date} to {end_date}")
            
            params = {
                'metadataPrefix': 'oai_dc',
                'set': 'cs',  # CSåˆ†ç±»
                'from': start_date,
                'until': end_date
            }
        
        try:
            root = self.make_oai_request('ListRecords', params)
            
            # è§£æè®°å½•
            records = root.findall('.//oai:record', NAMESPACES)
            logger.info(f"Processing {len(records)} records in this batch")
            
            for record in records:
                paper_data = self.extract_metadata_from_record(record)
                if paper_data:
                    papers.append(paper_data)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šæ•°æ®
            resumption_token_elem = root.find('.//oai:resumptionToken', NAMESPACES)
            next_token = resumption_token_elem.text if resumption_token_elem is not None else None
            
            logger.info(f"Extracted {len(papers)} valid CS papers from this batch")
            
            return papers, next_token
            
        except Exception as e:
            logger.error(f"Error fetching batch: {e}")
            return [], None
    
    def fetch_all_cs_papers(self, max_papers: Optional[int] = None) -> List[Dict]:
        """è·å–æ‰€æœ‰CSè®ºæ–‡å…ƒæ•°æ®"""
        all_papers = []
        processed_count = 0
        batch_count = 0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹ç»­ä¼ 
        resumption_token = self.load_checkpoint()
        if resumption_token:
            logger.info("Found checkpoint, resuming from previous session")
        
        start_time = time.time()
        
        try:
            while True:
                batch_count += 1
                logger.info(f"Processing batch {batch_count}...")
                
                papers, next_token = self.fetch_cs_papers_batch(resumption_token)
                
                if not papers:
                    logger.info("No more papers to fetch")
                    break
                
                all_papers.extend(papers)
                processed_count += len(papers)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                if batch_count % 10 == 0:  # æ¯10ä¸ªbatchä¿å­˜ä¸€æ¬¡
                    self.save_intermediate_results(all_papers, batch_count)
                
                logger.info(f"Total papers collected: {processed_count}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
                if max_papers and processed_count >= max_papers:
                    logger.info(f"Reached maximum paper limit: {max_papers}")
                    break
                
                # ä¿å­˜æ–­ç‚¹
                if next_token:
                    self.save_checkpoint(next_token)
                    resumption_token = next_token
                    
                    # éµå®ˆAPIé€Ÿç‡é™åˆ¶
                    time.sleep(1)
                else:
                    logger.info("Reached end of results")
                    break
                    
        except KeyboardInterrupt:
            logger.info("Process interrupted by user")
        except Exception as e:
            logger.error(f"Error during bulk fetch: {e}")
        finally:
            # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
            self.clear_checkpoint()
            
            elapsed_time = time.time() - start_time
            logger.info(f"Fetch completed. Total time: {elapsed_time:.2f}s, Total papers: {len(all_papers)}")
        
        return all_papers
    
    def save_intermediate_results(self, papers: List[Dict], batch_num: int):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        if not papers:
            return
            
        intermediate_file = os.path.join(self.output_dir, f"intermediate_batch_{batch_num}.parquet")
        df = pd.DataFrame(papers)
        df.to_parquet(intermediate_file, index=False)
        logger.info(f"Saved intermediate results to {intermediate_file}")
    
    def save_to_parquet(self, papers: List[Dict], filename: str = None):
        """ä¿å­˜åˆ°parquetæ–‡ä»¶"""
        if not papers:
            logger.warning("No papers to save")
            return
            
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cs_papers_6months_{timestamp}.parquet"
        
        filepath = os.path.join(self.output_dir, filename)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(papers)
        
        # æ•°æ®æ¸…ç†å’Œä¼˜åŒ–
        df = self.clean_dataframe(df)
        
        df.to_parquet(filepath, index=False, compression='snappy')
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(df, filepath)
    
    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†å’Œä¼˜åŒ–DataFrame"""
        logger.info("Cleaning and optimizing data...")
        
        # å»é‡ - åŸºäºarxiv_id
        initial_count = len(df)
        df = df.drop_duplicates(subset=['arxiv_id'], keep='last')
        logger.info(f"Removed {initial_count - len(df)} duplicates")
        
        # è½¬æ¢æ—¥æœŸç±»å‹
        df['date_submitted'] = pd.to_datetime(df['date_submitted'])
        df['last_updated'] = pd.to_datetime(df['last_updated'])
        df['fetch_time'] = pd.to_datetime(df['fetch_time'])
        
        # æ¸…ç†æ–‡æœ¬å­—æ®µ
        text_columns = ['title', 'abstract', 'authors']
        for col in text_columns:
            if col in df.columns:
                df[col] = df[col].str.strip()
                df[col] = df[col].replace('', None)  # ç©ºå­—ç¬¦ä¸²è½¬ä¸ºNone
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        df['arxiv_id'] = df['arxiv_id'].astype('category')
        
        # æŒ‰æ—¶é—´æ’åº
        df = df.sort_values('date_submitted', ascending=False)
        
        return df
    
    def print_statistics(self, df: pd.DataFrame, filepath: str):
        """æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("="*50)
        logger.info("ğŸ“Š Data Statistics")
        logger.info("="*50)
        logger.info(f"ğŸ“ File saved to: {filepath}")
        logger.info(f"ğŸ“„ Total papers: {len(df):,}")
        logger.info(f"ğŸ’¾ File size: {os.path.getsize(filepath) / 1024 / 1024:.2f} MB")
        
        # æ—¶é—´èŒƒå›´
        min_date = df['date_submitted'].min()
        max_date = df['date_submitted'].max()
        logger.info(f"ğŸ“… Date range: {min_date.date()} to {max_date.date()}")
        
        # åˆ†ç±»ç»Ÿè®¡
        if 'cs_categories' in df.columns:
            # å±•å¼€åˆ†ç±»ç»Ÿè®¡
            all_categories = []
            for cats in df['cs_categories'].dropna():
                all_categories.extend(cats.split('; '))
            
            from collections import Counter
            cat_counts = Counter(all_categories)
            
            logger.info(f"ğŸ·ï¸  Top 10 CS categories:")
            for cat, count in cat_counts.most_common(10):
                logger.info(f"   {cat}: {count:,}")
        
        # æŒ‰æœˆç»Ÿè®¡
        monthly_counts = df.groupby(df['date_submitted'].dt.to_period('M')).size()
        logger.info(f"ğŸ“ˆ Papers by month:")
        for month, count in monthly_counts.items():
            logger.info(f"   {month}: {count:,}")
        
        logger.info("="*50)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ‰¹é‡è·å–arXiv CSé¢†åŸŸè®ºæ–‡å…ƒæ•°æ®")
    parser.add_argument("--output-dir", default="arxiv_data", 
                        help="è¾“å‡ºç›®å½• (é»˜è®¤: arxiv_data)")
    parser.add_argument("--months", type=int, default=6,
                        help="è·å–æœ€è¿‘Nä¸ªæœˆçš„è®ºæ–‡ (é»˜è®¤: 6)")
    parser.add_argument("--max-papers", type=int,
                        help="æœ€å¤§è®ºæ–‡æ•°é‡é™åˆ¶")
    parser.add_argument("--filename", 
                        help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)")
    parser.add_argument("--verbose", action="store_true",
                        help="è¯¦ç»†æ—¥å¿—è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºè·å–å™¨
    fetcher = ArxivBulkFetcher(args.output_dir, args.months)
    
    try:
        logger.info("ğŸš€ Starting bulk CS papers metadata fetch...")
        logger.info(f"ğŸ“ Output directory: {args.output_dir}")
        logger.info(f"ğŸ“… Time range: last {args.months} months")
        if args.max_papers:
            logger.info(f"ğŸ“Š Max papers: {args.max_papers:,}")
        
        # è·å–è®ºæ–‡å…ƒæ•°æ®
        papers = fetcher.fetch_all_cs_papers(args.max_papers)
        
        if papers:
            # ä¿å­˜åˆ°parquet
            fetcher.save_to_parquet(papers, args.filename)
            logger.info("âœ… Bulk fetch completed successfully!")
        else:
            logger.warning("âš ï¸  No papers were fetched")
            
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())