#!/usr/bin/env python3
"""
ç»Ÿè®¡arXiv csåˆ†ç±»æœ€è¿‘Nå¤©çš„è®ºæ–‡æ•°é‡

æ”¯æŒå¤šç§csåˆ†ç±»ï¼Œæä¾›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
"""

import datetime
from typing import Dict, List, NamedTuple
import argparse
import csv
import logging
import sys
import statistics

import arxiv

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ”¯æŒçš„csåˆ†ç±»
CS_CATEGORIES = {
    'cs.AI': 'Artificial Intelligence',
    'cs.IR': 'Information Retrieval',
    'cs.CL': 'Computation and Language',
    'cs.CV': 'Computer Vision and Pattern Recognition',
    'cs.LG': 'Machine Learning',
    'cs.DB': 'Databases',
    'cs.DC': 'Distributed, Parallel, and Cluster Computing',
    'cs.DS': 'Data Structures and Algorithms',
    'cs.HC': 'Human-Computer Interaction',
    'cs.NE': 'Neural and Evolutionary Computing',
    'cs.RO': 'Robotics',
    'cs.SE': 'Software Engineering',
    'cs.SY': 'Systems and Control',
    'cs.CR': 'Cryptography and Security',
    'cs.GT': 'Computer Science and Game Theory',
    'cs.NI': 'Networking and Internet Architecture',
    'cs.OS': 'Operating Systems',
    'cs.PL': 'Programming Languages',
}


class PaperInfo(NamedTuple):
    """è®ºæ–‡ä¿¡æ¯"""
    paper_id: str
    title: str
    authors: str
    abstract: str
    published: datetime.date
    updated: datetime.date
    url: str


def format_date_chinese(date: datetime.date) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸä¸ºä¸­æ–‡æ˜¾ç¤º"""
    return f"{date.year}å¹´{date.month}æœˆ{date.day}æ—¥"


def get_papers_by_date_range(category: str = 'cs.AI', days: int = 7, max_results: int = 1000) -> Dict[str, List[PaperInfo]]:
    """
    è·å–æŒ‡å®šcsåˆ†ç±»ä¸‹æŒ‡å®šå¤©æ•°å†…çš„è®ºæ–‡ï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„
    
    Args:
        category: csåˆ†ç±»ï¼Œå¦‚cs.AI, cs.IRç­‰
        days: è·å–æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ï¼Œé»˜è®¤7å¤©
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤1000
        
    Returns:
        ä»¥æ—¥æœŸä¸ºkeyï¼Œè®ºæ–‡åˆ—è¡¨ä¸ºvalueçš„å­—å…¸
    """
    if category not in CS_CATEGORIES:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»: {category}ã€‚æ”¯æŒçš„åˆ†ç±»: {', '.join(CS_CATEGORIES.keys())}")
        
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=days - 1)  # æœ€è¿‘dayså¤©ï¼ˆåŒ…æ‹¬ä»Šå¤©ï¼‰
    
    category_name = CS_CATEGORIES[category]
    logger.info(f"è·å– {category} ({category_name}) åˆ†ç±»è®ºæ–‡ï¼Œæ—¶é—´èŒƒå›´: {format_date_chinese(start_date)} è‡³ {format_date_chinese(end_date)}")
    
    # æ„å»ºarXivæœç´¢æŸ¥è¯¢è¯­å¥
    query = f'cat:{category}'
    
    # åˆ›å»ºæœç´¢å¯¹è±¡
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    logger.info(f"å¼€å§‹ä»arXivè·å–è®ºæ–‡...")
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    papers_by_date = {}
    total_papers = 0
    
    try:
        for result in search.results():
            # è·å–è®ºæ–‡ä¿¡æ¯
            paper_id = result.get_short_id()
            title = result.title
            authors = ', '.join([author.name for author in result.authors])
            abstract = result.summary.replace('\n', ' ')
            published = result.published.date()
            updated = result.updated.date()
            url = result.entry_id
            
            # ä½¿ç”¨è®ºæ–‡çš„æ›´æ–°æ—¶é—´ä½œä¸ºåˆ†ç»„ä¾æ®
            paper_date = updated
            
            # åªç»Ÿè®¡æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è®ºæ–‡
            if start_date <= paper_date <= end_date:
                date_str = paper_date.strftime('%Y-%m-%d')
                if date_str not in papers_by_date:
                    papers_by_date[date_str] = []
                
                paper_info = PaperInfo(
                    paper_id=paper_id,
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    published=published,
                    updated=updated,
                    url=url
                )
                
                papers_by_date[date_str].append(paper_info)
                total_papers += 1
                
                logger.debug(f"è®ºæ–‡ {paper_id} ({paper_date}) å·²æ·»åŠ åˆ°ç»Ÿè®¡")
            else:
                logger.debug(f"è®ºæ–‡ {paper_id} æ—¥æœŸ {paper_date} ä¸åœ¨ç»Ÿè®¡èŒƒå›´å†…")
    except arxiv.UnexpectedEmptyPageError as e:
        logger.warning(f"arXiv APIè¿”å›ç©ºé¡µé¢ï¼Œå¯èƒ½å·²è·å–æ‰€æœ‰å¯ç”¨è®ºæ–‡: {e}")
    except Exception as e:
        logger.error(f"ä»arXivè·å–è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # ç»§ç»­å¤„ç†å·²ç»è·å–åˆ°çš„è®ºæ–‡
    
    logger.info(f"ä»arXivè·å–åˆ° {total_papers} ç¯‡åœ¨ç»Ÿè®¡èŒƒå›´å†…çš„è®ºæ–‡")
    return papers_by_date


def calculate_daily_stats(papers_by_date: Dict[str, List[PaperInfo]], days: int) -> Dict[str, float]:
    """
    è®¡ç®—æ¯æ—¥è®ºæ–‡æ•°é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        papers_by_date: æŒ‰æ—¥æœŸåˆ†ç»„çš„è®ºæ–‡å­—å…¸
        days: ç»Ÿè®¡å¤©æ•°
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«avg, p99, maxç­‰
    """
    # æ”¶é›†æ¯å¤©çš„è®ºæ–‡æ•°é‡ï¼ˆåŒ…æ‹¬0ç¯‡çš„æ—¥æœŸï¼‰
    daily_counts = []
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=days - 1)
    current_date = start_date
    
    while current_date <= end_date:
        date_str = current_date.strftime('%Y-%m-%d')
        count = len(papers_by_date.get(date_str, []))
        daily_counts.append(count)
        current_date += datetime.timedelta(days=1)
    
    stats = {}
    if daily_counts:
        stats['total'] = sum(daily_counts)
        stats['avg'] = statistics.mean(daily_counts)
        stats['median'] = statistics.median(daily_counts)
        stats['max'] = max(daily_counts)
        stats['min'] = min(daily_counts)
        
        # è®¡ç®—p99 (99th percentile)
        sorted_counts = sorted(daily_counts)
        if len(sorted_counts) > 1:
            p99_index = int(0.99 * (len(sorted_counts) - 1))
            stats['p99'] = sorted_counts[p99_index]
        else:
            stats['p99'] = sorted_counts[0] if sorted_counts else 0
            
        # æ ‡å‡†å·®
        if len(daily_counts) > 1:
            stats['std'] = statistics.stdev(daily_counts)
        else:
            stats['std'] = 0.0
            
        # æ´»è·ƒæ—¥æœŸç»Ÿè®¡
        active_days = [count for count in daily_counts if count > 0]
        stats['active_days'] = len(active_days)
        stats['active_avg'] = statistics.mean(active_days) if active_days else 0.0
    
    return stats


def print_statistics(papers_by_date: Dict[str, List[PaperInfo]], days: int, category: str = 'cs.AI'):
    """
    æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        papers_by_date: æŒ‰æ—¥æœŸåˆ†ç»„çš„è®ºæ–‡å­—å…¸
        days: ç»Ÿè®¡å¤©æ•°
        category: åˆ†ç±»åç§°
    """
    category_name = CS_CATEGORIES.get(category, category)
    stats = calculate_daily_stats(papers_by_date, days)
    
    print(f"\nğŸ“Š arXiv {category} ({category_name}) æœ€è¿‘ {days} å¤©è®ºæ–‡ç»Ÿè®¡")
    print("=" * 60)
    
    # æŒ‰æ—¥æœŸæ’åºæ˜¾ç¤ºæ¯æ—¥è¯¦æƒ…
    sorted_dates = sorted(papers_by_date.keys(), reverse=True)
    
    print("ğŸ“… æ¯æ—¥è®ºæ–‡æ•°é‡:")
    for date_str in sorted_dates:
        papers = papers_by_date[date_str]
        date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()
        chinese_date = format_date_chinese(date_obj)
        
        print(f"  {chinese_date} ({date_str}): {len(papers)} ç¯‡")
        
        if len(papers) > 0:
            # æ˜¾ç¤ºè®ºæ–‡æ ‡é¢˜ï¼ˆæœ€å¤šæ˜¾ç¤º2ç¯‡ï¼‰
            for i, paper in enumerate(papers[:2]):
                title = paper.title[:50] + '...' if len(paper.title) > 50 else paper.title
                print(f"    - {title}")
            
            if len(papers) > 2:
                print(f"    ... è¿˜æœ‰ {len(papers) - 2} ç¯‡è®ºæ–‡")
    
    # ç»Ÿè®¡æ²¡æœ‰è®ºæ–‡çš„æ—¥æœŸ
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=days - 1)
    current_date = start_date
    missing_dates = []
    
    while current_date <= end_date:
        date_str = current_date.strftime('%Y-%m-%d')
        if date_str not in papers_by_date:
            missing_dates.append(date_str)
        current_date += datetime.timedelta(days=1)
    
    if missing_dates and len(missing_dates) < days:
        print(f"\nğŸ“­ æ— è®ºæ–‡çš„æ—¥æœŸ ({len(missing_dates)} å¤©):")
        for date_str in missing_dates:
            date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d').date()
            chinese_date = format_date_chinese(date_obj)
            print(f"  {chinese_date} ({date_str})")
    
    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è®¡è®ºæ–‡æ•°: {stats['total']} ç¯‡")
    print(f"  å¹³å‡æ¯å¤©: {stats['avg']:.1f} ç¯‡")
    print(f"  ä¸­ä½æ•°: {stats['median']:.1f} ç¯‡")
    print(f"  æœ€å¤§å€¼: {stats['max']} ç¯‡")
    print(f"  æœ€å°å€¼: {stats['min']} ç¯‡")
    print(f"  P99: {stats['p99']} ç¯‡")
    print(f"  æ ‡å‡†å·®: {stats['std']:.1f}")
    print(f"  æ´»è·ƒæ—¥æœŸ: {stats['active_days']}/{days} å¤©")
    if stats['active_days'] > 0:
        print(f"  æ´»è·ƒæ—¥æœŸå¹³å‡: {stats['active_avg']:.1f} ç¯‡")


def export_to_csv(papers_by_date: Dict[str, List[PaperInfo]], filename: str):
    """
    å¯¼å‡ºç»Ÿè®¡æ•°æ®åˆ°CSVæ–‡ä»¶
    
    Args:
        papers_by_date: æŒ‰æ—¥æœŸåˆ†ç»„çš„è®ºæ–‡å­—å…¸
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['date', 'paper_count', 'paper_ids', 'paper_titles', 'authors', 'urls']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        
        sorted_dates = sorted(papers_by_date.keys(), reverse=True)
        for date_str in sorted_dates:
            papers = papers_by_date[date_str]
            paper_ids = '; '.join([paper.paper_id for paper in papers])
            paper_titles = '; '.join([paper.title for paper in papers])
            authors = '; '.join([paper.authors for paper in papers])
            urls = '; '.join([paper.url for paper in papers])
            
            writer.writerow({
                'date': date_str,
                'paper_count': len(papers),
                'paper_ids': paper_ids,
                'paper_titles': paper_titles,
                'authors': authors,
                'urls': urls
            })
    
    logger.info(f"ç»Ÿè®¡æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")


def list_categories():
    """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„åˆ†ç±»"""
    print("\nğŸ“š æ”¯æŒçš„arXiv csåˆ†ç±»:")
    print("=" * 50)
    for category, name in CS_CATEGORIES.items():
        print(f"  {category:<8} - {name}")
    print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ç»Ÿè®¡arXiv csåˆ†ç±»æœ€è¿‘Nå¤©çš„è®ºæ–‡æ•°é‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python scripts/stats_cs_ai_papers.py                          # ç»Ÿè®¡cs.AIæœ€è¿‘7å¤©
  python scripts/stats_cs_ai_papers.py --category cs.IR         # ç»Ÿè®¡cs.IRåˆ†ç±»
  python scripts/stats_cs_ai_papers.py --days 14 --category cs.CV  # ç»Ÿè®¡cs.CVæœ€è¿‘14å¤©  
  python scripts/stats_cs_ai_papers.py --list-categories         # åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„åˆ†ç±»
  python scripts/stats_cs_ai_papers.py --export stats.csv       # ç»Ÿè®¡å¹¶å¯¼å‡ºåˆ°CSVæ–‡ä»¶
        """
    )
    
    parser.add_argument(
        '--category',
        type=str,
        default='cs.AI',
        help='è¦ç»Ÿè®¡çš„csåˆ†ç±»ï¼Œé»˜è®¤cs.AI'
    )
    
    parser.add_argument(
        '--days', 
        type=int, 
        default=7, 
        help='ç»Ÿè®¡æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ï¼Œé»˜è®¤7å¤©'
    )
    
    parser.add_argument(
        '--max-results',
        type=int,
        default=1000,
        help='ä»arXivè·å–çš„æœ€å¤§è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤1000'
    )
    
    parser.add_argument(
        '--export',
        type=str,
        help='å¯¼å‡ºç»Ÿè®¡æ•°æ®åˆ°CSVæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--list-categories',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„åˆ†ç±»å¹¶é€€å‡º'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºåˆ†ç±»ï¼Œç›´æ¥è¿”å›
    if args.list_categories:
        list_categories()
        return
    
    if args.verbose:
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    try:
        # è·å–è®ºæ–‡æ•°æ®
        papers_by_date = get_papers_by_date_range(args.category, args.days, args.max_results)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_statistics(papers_by_date, args.days, args.category)
        
        # å¦‚æœæŒ‡å®šäº†å¯¼å‡ºé€‰é¡¹ï¼Œå¯¼å‡ºåˆ°CSV
        if args.export:
            export_to_csv(papers_by_date, args.export)
    
    except Exception as e:
        logger.error(f"ç»Ÿè®¡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()