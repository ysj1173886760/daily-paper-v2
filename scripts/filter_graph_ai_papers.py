#!/usr/bin/env python3
"""
ä½¿ç”¨LLMç­›é€‰Graph + AIç›¸å…³è®ºæ–‡
ä¸“æ³¨äºè¯†åˆ«å›¾ç¥ç»ç½‘ç»œã€çŸ¥è¯†å›¾è°±ã€GraphRAGç­‰é¢†åŸŸçš„ç»¼è¿°è®ºæ–‡å’Œé‡è¦ç ”ç©¶
"""

import pandas as pd
import os
import sys
import json
import time
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from json_repair import repair_json
from tqdm.asyncio import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from daily_paper.utils.call_llm import LLM, AsyncLLM


class GraphAIPaperFilter:
    def __init__(self, 
                 llm_base_url: Optional[str] = None,
                 llm_api_key: Optional[str] = None,
                 llm_model: str = "gpt-4",
                 temperature: float = 0.1,
                 batch_size: int = 10,
                 max_concurrent: int = 32,
                 rate_limit_delay: float = 0.1,
                 enable_concurrent: bool = True,
                 save_all_evaluations: bool = False,
                 save_token_stats: bool = False,
                 save_progress: bool = False,
                 save_report: bool = False,
                 resume_from_checkpoint: bool = False):
        """
        åˆå§‹åŒ–Graph + AIè®ºæ–‡ç­›é€‰å™¨
        
        Args:
            llm_base_url: LLM APIåŸºç¡€URL
            llm_api_key: LLM APIå¯†é’¥
            llm_model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¨¡å‹æ¸©åº¦å‚æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            rate_limit_delay: é€Ÿç‡é™åˆ¶å»¶è¿Ÿ
            enable_concurrent: æ˜¯å¦å¯ç”¨å¹¶å‘
            save_all_evaluations: æ˜¯å¦ä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœ
            save_token_stats: æ˜¯å¦ä¿å­˜tokenç»Ÿè®¡æ–‡ä»¶
            save_progress: æ˜¯å¦ä¿å­˜è¿›åº¦æ–‡ä»¶
            save_report: æ˜¯å¦ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        """
        self.config = {
            "llm_base_url": llm_base_url or os.getenv("LLM_BASE_URL"),
            "llm_api_key": llm_api_key or os.getenv("LLM_API_KEY"),
            "llm_model": llm_model,
            "temperature": temperature,
            "batch_size": batch_size,
            "max_concurrent": max_concurrent,
            "rate_limit_delay": rate_limit_delay,
            "enable_concurrent": enable_concurrent,
            "save_all_evaluations": save_all_evaluations,
            "save_token_stats": save_token_stats,
            "save_progress": save_progress,
            "save_report": save_report,
            "resume_from_checkpoint": resume_from_checkpoint
        }
        
        self._init_llm()
        self.criteria = self._define_filtering_criteria()
        
        # Tokenä½¿ç”¨ç»Ÿè®¡
        self.token_stats = {
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "total_tokens": 0,
            "api_calls": 0,
            "model": self.config["llm_model"],
            "per_call_stats": []
        }
    
    def _filter_by_time(self, df: pd.DataFrame, months: Optional[int] = None) -> pd.DataFrame:
        """
        æ ¹æ®æ—¶é—´è¿‡æ»¤è®ºæ–‡
        
        Args:
            df: è®ºæ–‡æ•°æ®DataFrame
            months: è¿‡æ»¤å¤šå°‘ä¸ªæœˆå†…çš„è®ºæ–‡ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤
            
        Returns:
            è¿‡æ»¤åçš„DataFrame
        """
        if months is None:
            return df
        
        # è®¡ç®—æ—¶é—´é˜ˆå€¼
        cutoff_date = datetime.now() - timedelta(days=months * 30)  # è¿‘ä¼¼æŒ‰30å¤©/æœˆè®¡ç®—
        
        # ç¡®ä¿date_submittedåˆ—æ˜¯datetimeç±»å‹
        if 'date_submitted' in df.columns:
            df = df.copy()
            df['date_submitted'] = pd.to_datetime(df['date_submitted'])
            
            # è¿‡æ»¤æ—¶é—´
            filtered_df = df[df['date_submitted'] >= cutoff_date]
            
            print(f"â° Time filtering: {months} months")
            print(f"   Cutoff date: {cutoff_date.strftime('%Y-%m-%d')}")
            print(f"   Before filtering: {len(df):,} papers")
            print(f"   After filtering: {len(filtered_df):,} papers")
            print(f"   Filtered out: {len(df) - len(filtered_df):,} papers")
            
            return filtered_df
        else:
            print("âš ï¸  Warning: 'date_submitted' column not found, skipping time filtering")
            return df
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        self.llm = LLM(
            llm_base_url=self.config["llm_base_url"],
            llm_api_key=self.config["llm_api_key"],
            llm_model=self.config["llm_model"],
        )
        self.async_llm = AsyncLLM(
            llm_base_url=self.config["llm_base_url"],
            llm_api_key=self.config["llm_api_key"],
            llm_model=self.config["llm_model"],
        )
    
    def _define_filtering_criteria(self) -> Dict[str, Any]:
        """å®šä¹‰ç­›é€‰æ ‡å‡†"""
        return {
            "target_domains": [
                "Graph Neural Networks (GNNs)",
                "Knowledge Graphs and AI",
                "Graph Foundation Models",
                "GraphRAG (Retrieval-Augmented Generation with Graphs)",
                "Graph Machine Learning",
                "AI applications on graph structures",
                "Graph-based reasoning and inference",
                "Multi-modal graph learning",
                "Graph transformers and attention mechanisms",
                "Large Language Models on graphs"
            ],
            "survey_types": [
                "Survey papers (ç³»ç»Ÿæ€§ç»¼è¿°)",
                "Review papers (å›é¡¾æ€§ç»¼è¿°)", 
                "Comprehensive studies (å…¨é¢æ€§ç ”ç©¶)",
                "Tutorial papers (æ•™ç¨‹æ€§è®ºæ–‡)",
                "Overview papers (æ¦‚è¿°æ€§è®ºæ–‡)",
                "Progress reports (è¿›å±•æŠ¥å‘Š)",
                "State-of-the-art reviews (æœ€æ–°æŠ€æœ¯ç»¼è¿°)"
            ],
            "survey_keywords": [
                # ç»¼è¿°ç±»å‹å…³é”®è¯ï¼ˆæ ‡é¢˜ä¸­çš„å¼ºæŒ‡ç¤ºè¯ï¼‰
                "survey", "review", "comprehensive study", "systematic review",
                "tutorial", "overview", "progress", "advances", "recent advances",
                "state-of-the-art", "comparison", "comparative study",
                
                # ç»¼è¿°æè¿°è¯ï¼ˆæ‘˜è¦ä¸­çš„æŒ‡ç¤ºè¯ï¼‰
                "systematic review", "comprehensive overview", "recent developments",
                "comparative analysis", "literature review", "field overview",
                "taxonomy", "classification", "categorization",
                
                # Graph + AI é¢†åŸŸå…³é”®è¯
                "graph neural network", "GNN", "graph transformer", 
                "knowledge graph", "KG", "graph foundation model",
                "graphrag", "graph-rag", "graph machine learning",
                "graph AI", "graph reasoning", "graph representation",
                "message passing", "graph attention", "graph convolution",
                "multi-modal graph", "heterogeneous graph", "temporal graph"
            ]
        }
    
    def _create_evaluation_prompt(self, paper: Dict[str, Any]) -> str:
        """åˆ›å»ºLLMè¯„ä¼°æç¤º"""
        
        domains_str = "\n".join(f"- {domain}" for domain in self.criteria["target_domains"])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç­›é€‰ä¸“å®¶ï¼Œä¸“æ³¨äºè¯†åˆ«Graph + AIé¢†åŸŸçš„ç»¼è¿°è®ºæ–‡ã€‚

**é‡è¦æé†’ï¼šæˆ‘åªå¯¹ç»¼è¿°/è°ƒç ”ç±»è®ºæ–‡æ„Ÿå…´è¶£ï¼Œè¯·ä¸¥æ ¼ç­›é€‰ï¼**

æˆ‘å…³æ³¨çš„é¢†åŸŸï¼š
{domains_str}

**ç»¼è¿°è®ºæ–‡è¯†åˆ«æ ‡å‡†ï¼š**
- æ ‡é¢˜åŒ…å«ï¼šSurvey, Review, Comprehensive Study, Tutorial, Overview, Progress, Advancesç­‰
- æ‘˜è¦æè¿°ï¼šç³»ç»Ÿæ€§å›é¡¾ã€å…¨é¢ç»¼è¿°ã€æœ€æ–°è¿›å±•ã€æ¯”è¾ƒåˆ†æã€é¢†åŸŸæ¦‚è§ˆç­‰
- å†…å®¹ç‰¹å¾ï¼šæ¶µç›–å¤šä¸ªæ–¹æ³•ã€æ¯”è¾ƒä¸åŒæŠ€æœ¯ã€æ€»ç»“é¢†åŸŸå‘å±•ã€æä¾›åˆ†ç±»æ¡†æ¶ç­‰

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{paper.get('title', 'N/A')}
æ‘˜è¦ï¼š{paper.get('abstract', 'N/A')}
åˆ†ç±»ï¼š{paper.get('cs_categories', 'N/A')}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†è¯„ä¼°ï¼ˆåªæœ‰ç»¼è¿°è®ºæ–‡æ‰åº”è¯¥å¾—åˆ°é«˜åˆ†ï¼‰ï¼š

1. **é¢†åŸŸç›¸å…³æ€§** (1-10åˆ†)ï¼š
   - 9-10åˆ†ï¼šGraph + AIæ ¸å¿ƒé¢†åŸŸçš„ç»¼è¿°
   - 7-8åˆ†ï¼šç›¸å…³é¢†åŸŸçš„ç»¼è¿°ï¼ˆå¦‚æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç»¼è¿°æ¶‰åŠå›¾æ–¹æ³•ï¼‰
   - 5-6åˆ†ï¼šè¾¹ç¼˜ç›¸å…³çš„ç»¼è¿°
   - 1-4åˆ†ï¼šä¸ç›¸å…³æˆ–éç»¼è¿°è®ºæ–‡

2. **ç»¼è¿°ç‰¹å¾** (1-10åˆ†)ï¼š
   - 9-10åˆ†ï¼šæ˜æ˜¾çš„ç»¼è¿°è®ºæ–‡ï¼Œæ ‡é¢˜å«Survey/Reviewç­‰ï¼Œæ‘˜è¦æè¿°ç³»ç»Ÿæ€§å›é¡¾
   - 7-8åˆ†ï¼šç»¼è¿°æ€§è´¨æ˜æ˜¾ï¼Œä½†å¯èƒ½ä¸æ˜¯ä¼ ç»Ÿç»¼è¿°æ ¼å¼
   - 5-6åˆ†ï¼šæœ‰ä¸€å®šç»¼è¿°æ€§è´¨ï¼Œä½†ä¸»è¦æ˜¯åŸåˆ›ç ”ç©¶
   - 3-4åˆ†ï¼šè½»å¾®ç»¼è¿°å†…å®¹ï¼Œä¸»è¦æ˜¯æ–¹æ³•è®ºæ–‡
   - 1-2åˆ†ï¼šå®Œå…¨ä¸æ˜¯ç»¼è¿°ï¼Œçº¯ç²¹çš„åŸåˆ›ç ”ç©¶æˆ–åº”ç”¨è®ºæ–‡

3. **ç»¼è¿°è´¨é‡** (1-10åˆ†)ï¼š
   - 9-10åˆ†ï¼šå…¨é¢æ·±å…¥çš„é¢†åŸŸç»¼è¿°ï¼Œå…·æœ‰é‡è¦å‚è€ƒä»·å€¼
   - 7-8åˆ†ï¼šè´¨é‡è¾ƒé«˜çš„ç»¼è¿°ï¼Œè¦†ç›–é¢å¹¿
   - 5-6åˆ†ï¼šä¸€èˆ¬æ€§ç»¼è¿°
   - 3-4åˆ†ï¼šç»¼è¿°è´¨é‡æœ‰é™
   - 1-2åˆ†ï¼šéç»¼è¿°æˆ–è´¨é‡å¾ˆä½

**è¯„åˆ¤è§„åˆ™ï¼š**
- åªæœ‰åŒæ—¶æ»¡è¶³"é¢†åŸŸç›¸å…³"å’Œ"ç¡®å®æ˜¯ç»¼è¿°"çš„è®ºæ–‡æ‰èƒ½è¢«æ ‡è®°ä¸ºç›®æ ‡è®ºæ–‡
- çº¯ç²¹çš„æ–¹æ³•è®ºæ–‡ã€åº”ç”¨è®ºæ–‡ã€å®éªŒè®ºæ–‡ç­‰ä¸€å¾‹æ’é™¤
- å®å¯æ¼æ‰ä¹Ÿä¸è¦è¯¯åˆ¤éç»¼è¿°è®ºæ–‡

è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "relevance_score": <1-10çš„æ•°å­—>,
    "survey_score": <1-10çš„æ•°å­—>,
    "quality_score": <1-10çš„æ•°å­—>,
    "overall_score": <å¹³å‡åˆ†>,
    "is_target_paper": <true/falseï¼Œå¿…é¡»æ˜¯ç»¼è¿°ä¸”overall_score >= 7.0>,
    "is_survey": <true/falseï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºç»¼è¿°è®ºæ–‡>,
    "reasoning": "<ç®€è¦è¯´æ˜è¯„åˆ¤ç†ç”±ï¼Œé‡ç‚¹è¯´æ˜æ˜¯å¦ä¸ºç»¼è¿°>",
    "survey_indicators": ["<ç»¼è¿°ç‰¹å¾1>", "<ç»¼è¿°ç‰¹å¾2>", "..."],
    "key_topics": ["<ä¸»è¦ä¸»é¢˜1>", "<ä¸»é¢˜2>", "..."]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
        
        return prompt
    
    async def _evaluate_paper_async(self, paper: Dict[str, Any], semaphore: asyncio.Semaphore) -> Dict[str, Any]:
        """å¼‚æ­¥è¯„ä¼°å•ç¯‡è®ºæ–‡"""
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
            try:
                # æ·»åŠ é€Ÿç‡é™åˆ¶å»¶è¿Ÿ
                if self.config["rate_limit_delay"] > 0:
                    await asyncio.sleep(self.config["rate_limit_delay"])
                
                prompt = self._create_evaluation_prompt(paper)
                
                # å¼‚æ­¥è°ƒç”¨LLMå¹¶æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
                response, usage_info = await self.async_llm.achat(
                    prompt,
                    temperature=self.config["temperature"],
                    return_usage=True,
                )
                
                # æ›´æ–°tokenç»Ÿè®¡ï¼ˆéœ€è¦åŒæ­¥ï¼‰
                self._update_token_stats(usage_info, paper.get("arxiv_id", "unknown"))
                
                # è§£æLLMå“åº”
                response = response.strip()
                
                # å°è¯•æ¸…ç†markdownæ ¼å¼
                if response.startswith("```json"):
                    response = response[7:]
                elif response.startswith("```"):
                    response = response[3:]
                    
                if response.endswith("```"):
                    response = response[:-3]
                
                response = response.strip()
                
                # å°è¯•è§£æJSONï¼Œå¦‚æœå¤±è´¥åˆ™è®°å½•è¯¦ç»†ä¿¡æ¯
                try:
                    result = json.loads(response)
                except json.JSONDecodeError as json_error:
                    # è®°å½•è§£æå¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
                    # print(f"ğŸš« JSON parsing failed for paper {paper.get('arxiv_id', 'unknown')}:")
                    # print(f"   Error: {str(json_error)}")
                    # print(f"   Response length: {len(response)}")
                    # print(f"   First 200 chars: {response[:200]!r}")
                    # if len(response) > 200:
                    #     print(f"   Last 100 chars: {response[-100:]!r}")
                    
                    # ä½¿ç”¨json_repairå°è¯•ä¿®å¤JSON
                    try:
                        fixed_response = repair_json(response)
                        result = json.loads(fixed_response)
                        # print(f"âœ… JSON parsing succeeded after repair_json fix")
                    except Exception as repair_error:
                        print(f"âŒ json_repair also failed: {str(repair_error)}")
                        raise json_error  # å¦‚æœä¿®å¤åè¿˜æ˜¯å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
                
                # æ·»åŠ è®ºæ–‡åŸºæœ¬ä¿¡æ¯å’Œtokenä½¿ç”¨ä¿¡æ¯
                result.update({
                    "arxiv_id": paper.get("arxiv_id"),
                    "title": paper.get("title"),
                    "authors": paper.get("authors"),
                    "cs_categories": paper.get("cs_categories"),
                    "date_submitted": str(paper.get("date_submitted", "")),
                    "evaluation_time": datetime.now().isoformat(),
                    "token_usage": usage_info
                })
                
                return result
                
            except Exception as e:
                print(f"âš ï¸  Error evaluating paper {paper.get('arxiv_id', 'unknown')}: {str(e)}")
                return {
                    "arxiv_id": paper.get("arxiv_id"),
                    "title": paper.get("title"),
                    "error": str(e),
                    "is_target_paper": False,
                    "overall_score": 0,
                    "evaluation_time": datetime.now().isoformat(),
                    "token_usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": self.config["llm_model"]}
                }
    
    def _update_token_stats(self, usage_info: Dict[str, Any], arxiv_id: str):
        """æ›´æ–°tokenä½¿ç”¨ç»Ÿè®¡"""
        self.token_stats["total_prompt_tokens"] += usage_info.get("prompt_tokens", 0)
        self.token_stats["total_completion_tokens"] += usage_info.get("completion_tokens", 0) 
        self.token_stats["total_tokens"] += usage_info.get("total_tokens", 0)
        self.token_stats["api_calls"] += 1
        
        # è®°å½•å•æ¬¡è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
        call_stat = {
            "arxiv_id": arxiv_id,
            "timestamp": datetime.now().isoformat(),
            **usage_info
        }
        self.token_stats["per_call_stats"].append(call_stat)
    
    def _estimate_cost(self) -> Dict[str, float]:
        """ä¼°ç®—APIè°ƒç”¨è´¹ç”¨ï¼ˆåŸºäºOpenAI GPT-4å®šä»·ï¼‰"""
        # GPT-4 å®šä»·ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰
        pricing = {
            "gpt-4": {"input": 0.03 / 1000, "output": 0.06 / 1000},  # æ¯1k tokençš„ä»·æ ¼
            "gpt-4-turbo": {"input": 0.01 / 1000, "output": 0.03 / 1000},
            "gpt-3.5-turbo": {"input": 0.0015 / 1000, "output": 0.002 / 1000},
        }
        
        model = self.token_stats["model"].lower()
        # å°è¯•åŒ¹é…æ¨¡å‹å®šä»·
        model_pricing = None
        for model_key, price in pricing.items():
            if model_key in model:
                model_pricing = price
                break
        
        if not model_pricing:
            model_pricing = pricing["gpt-4"]  # é»˜è®¤ä½¿ç”¨GPT-4å®šä»·
        
        input_cost = self.token_stats["total_prompt_tokens"] * model_pricing["input"]
        output_cost = self.token_stats["total_completion_tokens"] * model_pricing["output"]
        total_cost = input_cost + output_cost
        
        return {
            "input_cost_usd": round(input_cost, 4),
            "output_cost_usd": round(output_cost, 4),
            "total_cost_usd": round(total_cost, 4),
            "pricing_model": model_pricing,
            "cost_per_paper": round(total_cost / max(self.token_stats["api_calls"], 1), 4)
        }
    
    def get_token_summary(self) -> Dict[str, Any]:
        """è·å–tokenä½¿ç”¨æ€»ç»“"""
        cost_estimate = self._estimate_cost()
        
        return {
            "token_statistics": self.token_stats.copy(),
            "cost_estimate": cost_estimate,
            "efficiency_metrics": {
                "avg_prompt_tokens": round(self.token_stats["total_prompt_tokens"] / max(self.token_stats["api_calls"], 1), 1),
                "avg_completion_tokens": round(self.token_stats["total_completion_tokens"] / max(self.token_stats["api_calls"], 1), 1),
                "avg_total_tokens": round(self.token_stats["total_tokens"] / max(self.token_stats["api_calls"], 1), 1),
                "total_api_calls": self.token_stats["api_calls"]
            }
        }
    
    def _get_checkpoint_file_path(self, output_file: str) -> str:
        """ç”Ÿæˆcheckpointæ–‡ä»¶è·¯å¾„"""
        base_name = os.path.splitext(output_file)[0]
        return f"{base_name}_checkpoint.json"
    
    
    def _save_checkpoint(self, processed_papers: List[Dict[str, Any]], 
                        current_batch: int, total_batches: int,
                        output_file: str):
        """ä¿å­˜checkpointåˆ°JSONæ–‡ä»¶"""
        if not processed_papers:
            return
            
        checkpoint_file = self._get_checkpoint_file_path(output_file)
        
        # åˆ›å»ºcheckpointæ•°æ®
        checkpoint_data = {
            "metadata": {
                "current_batch": current_batch,
                "total_batches": total_batches,
                "total_processed": len(processed_papers),
                "timestamp": datetime.now().isoformat(),
                "token_stats": self.token_stats.copy()
            },
            "processed_papers": processed_papers
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ“‹ Checkpoint saved: {checkpoint_file} ({len(processed_papers)} papers)")
        except Exception as e:
            print(f"âŒ Failed to save checkpoint: {e}")
    
    def _load_checkpoint(self, output_file: str) -> Optional[Dict[str, Any]]:
        """ä»checkpointæ–‡ä»¶åŠ è½½è¿›åº¦"""
        checkpoint_file = self._get_checkpoint_file_path(output_file)
        
        if not os.path.exists(checkpoint_file):
            print("ğŸ“‹ No checkpoint found, starting fresh")
            return None
        
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            metadata = checkpoint_data["metadata"]
            processed_papers = checkpoint_data["processed_papers"]
            
            print(f"ğŸ“‹ Checkpoint loaded: {len(processed_papers)} papers processed, "
                  f"batch {metadata['current_batch']}/{metadata['total_batches']}")
            
            return checkpoint_data
        
        except Exception as e:
            print(f"âš ï¸  Failed to load checkpoint: {str(e)}")
            return None
    
    def _cleanup_checkpoint(self, output_file: str):
        """æ¸…ç†checkpointæ–‡ä»¶"""
        checkpoint_file = self._get_checkpoint_file_path(output_file)
        if os.path.exists(checkpoint_file):
            try:
                os.remove(checkpoint_file)
                print(f"ğŸ—‘ï¸  Checkpoint file cleaned up: {checkpoint_file}")
            except Exception as e:
                print(f"âš ï¸  Failed to cleanup checkpoint: {str(e)}")
    
    def evaluate_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ç¯‡è®ºæ–‡"""
        try:
            prompt = self._create_evaluation_prompt(paper)
            
            # è°ƒç”¨LLMå¹¶æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
            response, usage_info = self.llm.chat(
                prompt,
                temperature=self.config["temperature"],
                return_usage=True,
            )
            
            # æ›´æ–°tokenç»Ÿè®¡
            self._update_token_stats(usage_info, paper.get("arxiv_id", "unknown"))
            
            # è§£æLLMå“åº”
            # å°è¯•æå–JSONå†…å®¹
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.endswith("```"):
                response = response[:-3]
            if response.startswith("```"):
                response = response[3:]
                
            result = json.loads(response.strip())
            
            # æ·»åŠ è®ºæ–‡åŸºæœ¬ä¿¡æ¯å’Œtokenä½¿ç”¨ä¿¡æ¯
            result.update({
                "arxiv_id": paper.get("arxiv_id"),
                "title": paper.get("title"),
                "authors": paper.get("authors"),
                "cs_categories": paper.get("cs_categories"),
                "date_submitted": str(paper.get("date_submitted", "")),
                "evaluation_time": datetime.now().isoformat(),
                "token_usage": usage_info
            })
            
            return result
            
        except Exception as e:
            print(f"âš ï¸  Error evaluating paper {paper.get('arxiv_id', 'unknown')}: {str(e)}")
            return {
                "arxiv_id": paper.get("arxiv_id"),
                "title": paper.get("title"),
                "error": str(e),
                "is_target_paper": False,
                "overall_score": 0,
                "evaluation_time": datetime.now().isoformat(),
                "token_usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": self.config["llm_model"]}
            }
    
    async def _evaluate_papers_concurrent(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¹¶å‘è¯„ä¼°å¤šç¯‡è®ºæ–‡ - å¸¦è¿›åº¦æ¡æ˜¾ç¤º"""
        max_concurrent = self.config["max_concurrent"]
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [
            self._evaluate_paper_async(paper, semaphore) 
            for paper in papers
        ]
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ‰¹æ¬¡å†…è¿›åº¦ï¼Œå¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm(
            total=len(papers), 
            desc=f"ğŸ“ Processing", 
            unit="paper", 
            leave=False,
            ncols=80,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        # åŒ…è£…ä»»åŠ¡ä»¥æ›´æ–°è¿›åº¦æ¡
        async def track_progress(task):
            result = await task
            progress_bar.update(1)
            return result
        
        # æ‰§è¡Œå¸¦è¿›åº¦è·Ÿè¸ªçš„ä»»åŠ¡
        tracked_tasks = [track_progress(task) for task in tasks]
        results = await asyncio.gather(*tracked_tasks, return_exceptions=True)
        progress_bar.close()
        
        # ç»Ÿè®¡ç»“æœ
        successful_count = sum(1 for r in results if not isinstance(r, Exception) and not r.get("error"))
        error_count = len(results) - successful_count
        if error_count > 0:
            print(f"   âš ï¸  {error_count} errors occurred during processing")
        
        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âš ï¸  Exception in paper {papers[i].get('arxiv_id', 'unknown')}: {str(result)}")
                processed_results.append({
                    "arxiv_id": papers[i].get("arxiv_id"),
                    "title": papers[i].get("title"),
                    "error": str(result),
                    "is_target_paper": False,
                    "overall_score": 0,
                    "evaluation_time": datetime.now().isoformat(),
                    "token_usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": self.config["llm_model"]}
                })
            else:
                processed_results.append(result)
                
        return processed_results

    async def _evaluate_papers_concurrent_with_cleanup(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¸¦æ¸…ç†åŠŸèƒ½çš„å¹¶å‘è¯„ä¼°è®ºæ–‡"""
        try:
            results = await self._evaluate_papers_concurrent(papers)
            return results
        finally:
            # ç¡®ä¿AsyncClientæ­£ç¡®å…³é—­ï¼Œé˜²æ­¢event loopé”™è¯¯
            try:
                await self.async_llm.aclose()
            except Exception:
                pass  # å¿½ç•¥æ¸…ç†æ—¶çš„å¼‚å¸¸

    def filter_papers(self, data_file: str, output_file: Optional[str] = None, 
                     max_papers: Optional[int] = None, start_index: int = 0,
                     months_filter: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        ç­›é€‰è®ºæ–‡
        
        Args:
            data_file: è¾“å…¥çš„parquetæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_papers: æœ€å¤§å¤„ç†è®ºæ–‡æ•°é‡
            start_index: å¼€å§‹å¤„ç†çš„ç´¢å¼•
            months_filter: è¿‡æ»¤æœ€è¿‘Nä¸ªæœˆçš„è®ºæ–‡
        """
        print(f"ğŸ“– Loading papers from: {data_file}")
        df = pd.read_parquet(data_file)
        print(f"ğŸ“Š Total papers in file: {len(df):,}")
        
        # åº”ç”¨æ—¶é—´è¿‡æ»¤
        df = self._filter_by_time(df, months_filter)
        
        # åº”ç”¨ç´¢å¼•å’Œæ•°é‡é™åˆ¶
        if max_papers:
            end_index = min(start_index + max_papers, len(df))
            df = df.iloc[start_index:end_index]
            print(f"ğŸ”¢ Limited to {max_papers} papers starting from index {start_index}")
        else:
            df = df.iloc[start_index:]
            if start_index > 0:
                print(f"ğŸ”¢ Starting from index {start_index}")
        
        print(f"ğŸ“‹ Final paper count to analyze: {len(df):,}")
        if len(df) == 0:
            print("âŒ No papers to analyze after filtering. Exiting.")
            return []
        
        # æ˜¾ç¤ºå¤„ç†æ¨¡å¼å’ŒcheckpointçŠ¶æ€
        if self.config["enable_concurrent"]:
            print(f"ğŸš€ Using concurrent processing with max {self.config['max_concurrent']} concurrent requests")
            if self.config["save_progress"]:
                print("ğŸ“‹ Checkpoint enabled: progress will be saved after each batch")
                if self.config["resume_from_checkpoint"]:
                    print("ğŸ”„ Resume mode: will attempt to load from checkpoint if exists")
            else:
                print("ğŸ“‹ Checkpoint disabled: no progress saving")
            return self._filter_papers_concurrent(df, output_file, start_index)
        else:
            print("ğŸ”„ Using sequential processing")
            return self._filter_papers_sequential(df, output_file, start_index)
    
    def _filter_papers_concurrent(self, df: pd.DataFrame, output_file: Optional[str], start_index: int = 0) -> List[Dict[str, Any]]:
        """
        å¹¶å‘æ¨¡å¼ç­›é€‰è®ºæ–‡ - åŸºäºbatchçš„å¤„ç†å’Œcheckpointæœºåˆ¶
        
        ç‰¹æ€§:
        - Batchå†…å¹¶å‘å¤„ç†ï¼Œbatché—´é¡ºåºæ‰§è¡Œ
        - æ¯ä¸ªbatchå®Œæˆåä¿å­˜checkpoint
        - æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        - å®æ—¶æ˜¾ç¤ºbatchå†…å¤„ç†è¿›åº¦æ¡
        """
        # ç¡®å®šå®é™…çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºcheckpointï¼‰
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            actual_output_file = f"arxiv_data/graph_ai_papers_{timestamp}.json"
        else:
            actual_output_file = output_file
            
        # æ˜¾ç¤ºcheckpointä¿¡æ¯
        if self.config["save_progress"]:
            checkpoint_file = self._get_checkpoint_file_path(actual_output_file)
            print(f"   Checkpoint location: {checkpoint_file}")
        
        results = []
        relevant_papers = []
        batch_size = self.config["batch_size"]
        
        # è®¡ç®—æ‰¹æ¬¡ä¿¡æ¯
        total_batches = (len(df) + batch_size - 1) // batch_size
        current_batch_start = 0
        
        # å°è¯•ä»checkpointæ¢å¤ (åªæœ‰å½“save_progresså¯ç”¨æ—¶)
        if self.config["resume_from_checkpoint"] and self.config["save_progress"]:
            checkpoint = self._load_checkpoint(actual_output_file)
            if checkpoint:
                results = checkpoint["processed_papers"]
                relevant_papers = [p for p in results if p.get("is_target_paper", False)]
                current_batch_start = checkpoint["metadata"]["current_batch"] * batch_size
                
                # æ¢å¤tokenç»Ÿè®¡
                if "token_stats" in checkpoint["metadata"]:
                    saved_stats = checkpoint["metadata"]["token_stats"]
                    self.token_stats.update(saved_stats)
                
                print(f"ğŸ”„ Resuming from batch {checkpoint['metadata']['current_batch'] + 1}/{total_batches}")
                print(f"ğŸ“Š Already processed {len(results)} papers, {len(relevant_papers)} relevant")
        
        # æŒ‰æ‰¹å¤„ç† - batchå†…å¹¶å‘ï¼Œbatché—´é¡ºåºæ‰§è¡Œ
        for batch_idx in range(current_batch_start // batch_size, total_batches):
            try:
                batch_start = batch_idx * batch_size
                batch_end = min(batch_start + batch_size, len(df))
                batch_df = df.iloc[batch_start:batch_end]
                
                print(f"ğŸ”„ Processing batch {batch_idx + 1}/{total_batches}: papers {batch_start + 1}-{batch_end}")
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                batch_papers = [row.to_dict() for _, row in batch_df.iterrows()]
                
                # å¼‚æ­¥è¯„ä¼°è¿™ä¸€æ‰¹è®ºæ–‡ (batchå†…å¹¶å‘ï¼Œå¸¦è¿›åº¦æ¡)
                batch_results = asyncio.run(self._evaluate_papers_concurrent_with_cleanup(batch_papers))
                
            except Exception as batch_error:
                print(f"âŒ Critical error in batch {batch_idx + 1}: {str(batch_error)}")
                print(f"âš ï¸  Skipping this batch and continuing with next batch...")
                
                # åˆ›å»ºç©ºçš„batchç»“æœæ¥ä¿æŒä¸€è‡´æ€§
                batch_results = []
                for paper in batch_papers if 'batch_papers' in locals() else []:
                    batch_results.append({
                        "arxiv_id": paper.get("arxiv_id", "unknown"),
                        "title": paper.get("title", "Unknown"),
                        "error": f"Batch processing error: {str(batch_error)}",
                        "is_target_paper": False,
                        "overall_score": 0,
                        "evaluation_time": datetime.now().isoformat(),
                        "token_usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "model": self.config["llm_model"]}
                    })
                
                # ç»§ç»­åˆ°ä¸‹ä¸€ä¸ªbatch
                continue
            
            # å¤„ç†æ‰¹æ¬¡ç»“æœ
            batch_relevant_count = 0
            for result in batch_results:
                try:
                    results.append(result)
                    
                    if result.get("is_target_paper", False):
                        relevant_papers.append(result)
                        batch_relevant_count += 1
                        
                        # å®‰å…¨çš„å­—ç¬¦ä¸²æ ¼å¼åŒ–
                        title = result.get('title', 'Unknown')
                        if isinstance(title, str):
                            title_display = title[:80] + "..." if len(title) > 80 else title
                        else:
                            title_display = str(title)[:80] + "..."
                        
                        # å®‰å…¨çš„åˆ†æ•°æ ¼å¼åŒ–
                        try:
                            score = float(result.get('overall_score', 0))
                            score_display = f"{score:.1f}"
                        except (ValueError, TypeError):
                            score_display = str(result.get('overall_score', 'N/A'))
                        
                        # å®‰å…¨çš„ä¸»é¢˜åˆ—è¡¨æ ¼å¼åŒ–
                        topics = result.get('key_topics', [])
                        if isinstance(topics, list):
                            topics_display = topics
                        else:
                            topics_display = [str(topics)]
                        
                        print(f"âœ… Found relevant paper: {title_display}")
                        print(f"   Score: {score_display}, Topics: {topics_display}")
                        
                except Exception as format_error:
                    print(f"âš ï¸  Error formatting result display for paper {result.get('arxiv_id', 'unknown')}: {str(format_error)}")
                    # ä»ç„¶æ·»åŠ åˆ°ç»“æœä¸­ï¼Œä½†ä½¿ç”¨ç®€åŒ–æ˜¾ç¤º
                    results.append(result)
                    if result.get("is_target_paper", False):
                        relevant_papers.append(result)
                        batch_relevant_count += 1
                        print(f"âœ… Found relevant paper (display error): {result.get('arxiv_id', 'unknown')}")
            
            # æ‰¹æ¬¡å®Œæˆåç«‹å³ä¿å­˜checkpoint (å¦‚æœå¯ç”¨äº†save_progress)
            if self.config["save_progress"]:
                self._save_checkpoint(results, batch_idx, total_batches, actual_output_file)
            
            # æ˜¾ç¤ºæ‰¹æ¬¡ç»Ÿè®¡
            print(f"ğŸ“Š Batch {batch_idx + 1}/{total_batches} completed. Found {batch_relevant_count} relevant in this batch.")
            
            # å®‰å…¨çš„ç™¾åˆ†æ¯”è®¡ç®—
            try:
                if len(results) > 0:
                    percentage = len(relevant_papers) / len(results) * 100
                    print(f"ğŸ“ˆ Total: {len(relevant_papers)}/{len(results)} relevant ({percentage:.1f}%)")
                else:
                    print(f"ğŸ“ˆ Total: {len(relevant_papers)}/0 relevant (N/A%)")
            except Exception as calc_error:
                print(f"ğŸ“ˆ Total: {len(relevant_papers)}/{len(results)} relevant (calc error: {str(calc_error)})")
            
            if self.token_stats["api_calls"] > 0:
                avg_tokens = self.token_stats["total_tokens"] / self.token_stats["api_calls"]
                estimated_cost = self._estimate_cost()["total_cost_usd"]
                print(f"ğŸ’° Token usage: {self.token_stats['total_tokens']:,} total, {avg_tokens:.1f} avg, ${estimated_cost:.4f} cost")
                
            print("-" * 50)
        
        # æœ€ç»ˆä¿å­˜å¹¶æ¸…ç†checkpoint
        self._save_final_results(results, relevant_papers, actual_output_file)
        # åªæœ‰å¯ç”¨äº†save_progressæ—¶æ‰æ¸…ç†checkpoint
        if self.config["save_progress"]:
            self._cleanup_checkpoint(actual_output_file)
        
        return relevant_papers
    
    def _filter_papers_sequential(self, df: pd.DataFrame, output_file: Optional[str], start_index: int) -> List[Dict[str, Any]]:
        """é¡ºåºæ¨¡å¼ç­›é€‰è®ºæ–‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        results = []
        relevant_papers = []
        
        for idx, (_, paper) in enumerate(df.iterrows()):
            print(f"ğŸ” Evaluating paper {idx + 1}/{len(df)}: {paper['arxiv_id']}")
            
            result = self.evaluate_paper(paper.to_dict())
            results.append(result)
            
            if result.get("is_target_paper", False):
                relevant_papers.append(result)
                print(f"âœ… Found relevant paper: {result.get('title', 'Unknown')[:80]}...")
                print(f"   Score: {result.get('overall_score', 0):.1f}, Topics: {result.get('key_topics', [])}")
            
            # æ‰¹é‡ä¿å­˜è¿›åº¦
            if (idx + 1) % self.config["batch_size"] == 0:
                self._save_progress(results, relevant_papers, output_file, start_index + idx + 1)
                
            # é¿å…APIè°ƒç”¨è¿‡å¿«
            time.sleep(0.5)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(results, relevant_papers, output_file)
        
        return relevant_papers
    
    def _save_progress(self, all_results: List[Dict[str, Any]], 
                      relevant_papers: List[Dict[str, Any]], 
                      output_file: Optional[str], current_idx: int):
        """ä¿å­˜ä¸­é—´è¿›åº¦"""
        if not output_file or not self.config["save_progress"]:
            return
            
        base_name = os.path.splitext(output_file)[0]
        
        # ä¿å­˜è¿›åº¦æ–‡ä»¶
        progress_file = f"{base_name}_progress_{current_idx}.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump({
                "processed_count": len(all_results),
                "relevant_count": len(relevant_papers),
                "last_index": current_idx,
                "timestamp": datetime.now().isoformat(),
                "all_results": all_results,
                "relevant_papers": relevant_papers
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Progress saved: {progress_file} (Found {len(relevant_papers)} relevant papers)")
    
    def _save_final_results(self, all_results: List[Dict[str, Any]], 
                           relevant_papers: List[Dict[str, Any]], 
                           output_file: Optional[str]):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"arxiv_data/graph_ai_papers_{timestamp}.json"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # è·å–tokenä½¿ç”¨æ€»ç»“
        token_summary = self.get_token_summary()
        
        # ä¿å­˜ç­›é€‰å‡ºçš„ç›¸å…³è®ºæ–‡ï¼ˆä¸»è¦è¾“å‡ºæ–‡ä»¶ï¼Œå§‹ç»ˆä¿å­˜ï¼‰
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": {
                    "total_evaluated": len(all_results),
                    "relevant_found": len(relevant_papers),
                    "success_rate": len(relevant_papers) / len(all_results) if all_results else 0,
                    "evaluation_time": datetime.now().isoformat(),
                    "criteria": self.criteria,
                    "token_usage": token_summary if self.config["save_token_stats"] else {
                        "total_api_calls": self.token_stats["api_calls"],
                        "total_tokens": self.token_stats["total_tokens"],
                        "estimated_cost_usd": self._estimate_cost()["total_cost_usd"]
                    }
                },
                "papers": relevant_papers
            }, f, ensure_ascii=False, indent=2)
        
        # å¯é€‰ï¼šä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœ
        all_results_file = None
        if self.config["save_all_evaluations"]:
            all_results_file = output_file.replace('.json', '_all_evaluations.json')
            with open(all_results_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # å¯é€‰ï¼šä¿å­˜è¯¦ç»†çš„tokenç»Ÿè®¡
        token_stats_file = None
        if self.config["save_token_stats"]:
            token_stats_file = output_file.replace('.json', '_token_stats.json')
            with open(token_stats_file, 'w', encoding='utf-8') as f:
                json.dump(token_summary, f, ensure_ascii=False, indent=2)
        
        # å¯é€‰ï¼šç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        report_file = None
        if self.config["save_report"]:
            report_file = output_file.replace('.json', '_report.txt')
            self._generate_report(all_results, relevant_papers, report_file)
        
        print(f"âœ… Results saved:")
        print(f"   ğŸ¯ Relevant papers: {output_file}")
        if all_results_file:
            print(f"   ğŸ“„ All evaluations: {all_results_file}")
        if token_stats_file:
            print(f"   ğŸ“Š Token statistics: {token_stats_file}")
        if report_file:
            print(f"   ğŸ“‹ Text report: {report_file}")
        # å®‰å…¨çš„ç™¾åˆ†æ¯”è®¡ç®—
        try:
            if len(all_results) > 0:
                success_rate = len(relevant_papers) / len(all_results) * 100
                print(f"   ğŸ“ˆ Found {len(relevant_papers)}/{len(all_results)} relevant survey papers ({success_rate:.1f}%)")
            else:
                print(f"   ğŸ“ˆ Found {len(relevant_papers)}/0 relevant survey papers (N/A%)")
        except Exception as calc_error:
            print(f"   ğŸ“ˆ Found {len(relevant_papers)}/{len(all_results)} relevant survey papers (calc error)")
        
        # æ˜¾ç¤ºtokenä½¿ç”¨ç»Ÿè®¡
        cost_info = token_summary["cost_estimate"]
        efficiency = token_summary["efficiency_metrics"]
        print(f"\nğŸ’° Token Usage Summary:")
        print(f"   Total API calls: {efficiency['total_api_calls']}")
        print(f"   Total tokens used: {self.token_stats['total_tokens']:,}")
        print(f"   Average tokens per call: {efficiency['avg_total_tokens']}")
        print(f"   Estimated cost: ${cost_info['total_cost_usd']:.4f} USD")
        print(f"   Cost per paper: ${cost_info['cost_per_paper']:.4f} USD")
    
    def _generate_report(self, all_results: List[Dict[str, Any]], 
                        relevant_papers: List[Dict[str, Any]], 
                        report_file: str):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        token_summary = self.get_token_summary()
        cost_info = token_summary["cost_estimate"]
        efficiency = token_summary["efficiency_metrics"]
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Graph + AI Paper Filtering Report\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"ğŸ“Š Summary Statistics:\n")
            f.write(f"   Total papers evaluated: {len(all_results)}\n")
            f.write(f"   Relevant papers found: {len(relevant_papers)}\n")
            # å®‰å…¨çš„æˆåŠŸç‡è®¡ç®—
            try:
                if len(all_results) > 0:
                    success_rate = len(relevant_papers) / len(all_results) * 100
                    f.write(f"   Success rate: {success_rate:.1f}%\n\n")
                else:
                    f.write(f"   Success rate: N/A%\n\n")
            except Exception:
                f.write(f"   Success rate: calculation error\n\n")
            
            # æ·»åŠ tokenä½¿ç”¨ç»Ÿè®¡
            f.write(f"ğŸ’° Token Usage & Cost Analysis:\n")
            f.write(f"   Total API calls: {efficiency['total_api_calls']}\n")
            f.write(f"   Total tokens: {self.token_stats['total_tokens']:,}\n")
            f.write(f"     - Prompt tokens: {self.token_stats['total_prompt_tokens']:,}\n")
            f.write(f"     - Completion tokens: {self.token_stats['total_completion_tokens']:,}\n")
            f.write(f"   Average tokens per call: {efficiency['avg_total_tokens']:.1f}\n")
            f.write(f"   Estimated cost: ${cost_info['total_cost_usd']:.4f} USD\n")
            f.write(f"   Cost per paper: ${cost_info['cost_per_paper']:.4f} USD\n")
            f.write(f"   Model used: {self.token_stats['model']}\n\n")
            
            if relevant_papers:
                # ç¡®ä¿scoreséƒ½æ˜¯æ•°å­—ç±»å‹ï¼Œé¿å…dtypeå…¼å®¹æ€§é—®é¢˜
                scores = []
                for p in relevant_papers:
                    score = p.get('overall_score', 0)
                    try:
                        # å¼ºåˆ¶è½¬æ¢ä¸ºfloat
                        if isinstance(score, str):
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–æ•°å­—
                            import re
                            numbers = re.findall(r'\d+\.?\d*', str(score))
                            score = float(numbers[0]) if numbers else 0.0
                        else:
                            score = float(score)
                    except (ValueError, TypeError):
                        score = 0.0
                    scores.append(score)
                
                f.write(f"ğŸ¯ Relevant Papers Analysis:\n")
                if scores:
                    # ä½¿ç”¨Pythonå†…ç½®å‡½æ•°è®¡ç®—ï¼Œé¿å…numpyçš„dtypeé—®é¢˜
                    avg_score = sum(scores) / len(scores)
                    f.write(f"   Average score: {avg_score:.2f}\n")
                    f.write(f"   Score range: {min(scores):.1f} - {max(scores):.1f}\n\n")
                else:
                    f.write(f"   No valid scores available\n\n")
                
                f.write(f"ğŸ“ Top Papers (by score):\n")
                
                # å®‰å…¨çš„æ’åºï¼Œå¤„ç†å¯èƒ½çš„æ•°æ®ç±»å‹é—®é¢˜
                def safe_score_key(paper):
                    score = paper.get('overall_score', 0)
                    try:
                        if isinstance(score, str):
                            import re
                            numbers = re.findall(r'\d+\.?\d*', str(score))
                            return float(numbers[0]) if numbers else 0.0
                        else:
                            return float(score)
                    except (ValueError, TypeError):
                        return 0.0
                
                sorted_papers = sorted(relevant_papers, key=safe_score_key, reverse=True)
                
                for i, paper in enumerate(sorted_papers[:10], 1):
                    f.write(f"\n{i}. {paper.get('title', 'Unknown')}\n")
                    f.write(f"   ArXiv ID: {paper.get('arxiv_id', 'N/A')}\n")
                    # å®‰å…¨åœ°æ˜¾ç¤ºåˆ†æ•°
                    score = safe_score_key(paper)
                    f.write(f"   Score: {score:.1f}\n")
                    # å®‰å…¨åœ°å¤„ç†åˆ—è¡¨å­—æ®µ
                    topics = paper.get('key_topics', [])
                    if isinstance(topics, list):
                        topics_str = ', '.join(topics)
                    else:
                        topics_str = str(topics) if topics else 'N/A'
                    f.write(f"   Topics: {topics_str}\n")
                    f.write(f"   Reasoning: {paper.get('reasoning', 'N/A')}\n")
                    if paper.get('token_usage'):
                        usage = paper['token_usage']
                        f.write(f"   Token usage: {usage.get('total_tokens', 0)} total\n")
        
        print(f"ğŸ“‹ Report generated: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LLMç­›é€‰Graph + AIç›¸å…³è®ºæ–‡")
    
    # è¾“å…¥è¾“å‡ºå‚æ•°
    parser.add_argument("--input", 
                        help="è¾“å…¥çš„parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", 
                        help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    
    # æ•°æ®è¿‡æ»¤å‚æ•°
    parser.add_argument("--max_papers", type=int,
                        help="æœ€å¤§å¤„ç†è®ºæ–‡æ•°é‡")
    parser.add_argument("--start_index", type=int, default=0,
                        help="å¼€å§‹å¤„ç†çš„ç´¢å¼•ä½ç½®")
    parser.add_argument("--months", type=int,
                        help="è¿‡æ»¤æœ€è¿‘Nä¸ªæœˆå†…çš„è®ºæ–‡ï¼ˆä¾‹å¦‚ï¼š6è¡¨ç¤ºæœ€è¿‘6ä¸ªæœˆï¼‰")
    
    # LLMé…ç½®å‚æ•°
    parser.add_argument("--llm_base_url",
                        help="LLM APIåŸºç¡€URLï¼ˆä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡LLM_BASE_URLï¼‰")
    parser.add_argument("--llm_api_key",
                        help="LLM APIå¯†é’¥ï¼ˆä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡LLM_API_KEYï¼‰") 
    parser.add_argument("--llm_model", default="gpt-4",
                        help="ä½¿ç”¨çš„LLMæ¨¡å‹ï¼ˆé»˜è®¤ï¼šgpt-4ï¼‰")
    parser.add_argument("--temperature", type=float, default=0.1,
                        help="LLMæ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ï¼š0.1ï¼‰")
    
    # å¹¶å‘å¤„ç†å‚æ•°  
    parser.add_argument("--max_concurrent", type=int, default=32,
                        help="æœ€å¤§å¹¶å‘æ•°é‡ï¼ˆé»˜è®¤32ï¼‰")
    parser.add_argument("--disable_concurrent", action="store_true",
                        help="ç¦ç”¨å¹¶å‘å¤„ç†ï¼Œä½¿ç”¨é¡ºåºæ¨¡å¼")
    parser.add_argument("--rate_limit_delay", type=float, default=0.1,
                        help="å¹¶å‘æ—¶çš„é€Ÿç‡é™åˆ¶å»¶è¿Ÿï¼Œå•ä½ç§’ï¼ˆé»˜è®¤0.1ï¼‰")
    
    # æ‰¹å¤„ç†å‚æ•°
    parser.add_argument("--batch_size", type=int, default=10,
                        help="æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤10ï¼‰")
    
    # è¾“å‡ºæ§åˆ¶å‚æ•°
    parser.add_argument("--save_all_evaluations", action="store_true",
                        help="ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„è¯„ä¼°ç»“æœï¼ˆåŒ…æ‹¬è¢«æ‹’ç»çš„ï¼‰")
    parser.add_argument("--save_token_stats", action="store_true", 
                        help="ä¿å­˜è¯¦ç»†çš„tokenä½¿ç”¨ç»Ÿè®¡æ–‡ä»¶")
    parser.add_argument("--save_progress", action="store_true",
                        help="å¯ç”¨checkpointåŠŸèƒ½ï¼šæ¯ä¸ªbatchåä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--save_report", action="store_true",
                        help="ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š")
    
    parser.add_argument("--resume", action="store_true",
                        help="ä»checkpointæ¢å¤å¤„ç†ï¼ˆéœ€è¦åŒæ—¶å¯ç”¨--save_progressï¼‰")
    
    args = parser.parse_args()
    
    # é»˜è®¤è¾“å…¥æ–‡ä»¶
    if not args.input:
        data_dir = "arxiv_data"
        if os.path.exists(data_dir):
            parquet_files = [f for f in os.listdir(data_dir) if f.endswith('.parquet') and 'cs_papers' in f]
            if parquet_files:
                args.input = os.path.join(data_dir, sorted(parquet_files)[-1])
                print(f"ğŸ” Using latest data file: {args.input}")
            else:
                print(f"âŒ No parquet files found in {data_dir}")
                return 1
        else:
            print(f"âŒ Data directory not found: {data_dir}")
            return 1
    
    if not os.path.exists(args.input):
        print(f"âŒ Input file not found: {args.input}")
        return 1
    
    try:
        # åˆå§‹åŒ–ç­›é€‰å™¨ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        filter_engine = GraphAIPaperFilter(
            llm_base_url=args.llm_base_url,
            llm_api_key=args.llm_api_key,
            llm_model=args.llm_model,
            temperature=args.temperature,
            batch_size=args.batch_size,
            max_concurrent=args.max_concurrent,
            rate_limit_delay=args.rate_limit_delay,
            enable_concurrent=not args.disable_concurrent,
            save_all_evaluations=args.save_all_evaluations,
            save_token_stats=args.save_token_stats,
            save_progress=args.save_progress,
            save_report=args.save_report,
            resume_from_checkpoint=args.resume
        )
        
        # æ‰§è¡Œç­›é€‰
        relevant_papers = filter_engine.filter_papers(
            data_file=args.input,
            output_file=args.output,
            max_papers=args.max_papers,
            start_index=args.start_index,
            months_filter=args.months
        )
        
        print(f"\nğŸ‰ Filtering completed successfully!")
        print(f"Found {len(relevant_papers)} relevant papers in Graph + AI domains.")
        
        # æœ€ç»ˆæ¸…ç†AsyncClient
        try:
            import asyncio
            # æ¸…ç†å¼‚æ­¥å®¢æˆ·ç«¯
            try:
                asyncio.run(self.async_llm.aclose())
            except Exception:
                pass
        except Exception:
            pass  # å¿½ç•¥æ¸…ç†å¼‚å¸¸
        
        return 0
        
    except Exception as e:
        print(f"âŒ Error during filtering: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
