#!/usr/bin/env python3
"""
ä»ç­›é€‰ç»“æœä¸­æå–åŒ…å«"graph"å…³é”®è¯çš„è®ºæ–‡
è¾“å‡ºæ ¼å¼ï¼š
## è®ºæ–‡æ ‡é¢˜
è®ºæ–‡æ‘˜è¦å†…å®¹
"""

import json
import argparse
import os
import re
import pandas as pd
from typing import List, Dict, Any


def extract_graph_papers(json_file: str, parquet_file: str = None, output_file: str = None):
    """
    ä»JSONæ–‡ä»¶ä¸­æå–æ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«"graph"çš„è®ºæ–‡
    
    Args:
        json_file: ç­›é€‰ç»“æœJSONæ–‡ä»¶è·¯å¾„
        parquet_file: åŒ…å«abstractçš„parquetæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°
    """
    print(f"ğŸ“– Loading filtered papers from: {json_file}")
    
    try:
        # åŠ è½½ç­›é€‰ç»“æœ
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ£€æŸ¥JSONç»“æ„
        if isinstance(data, dict) and "papers" in data:
            papers = data["papers"]
            print(f"ğŸ“Š JSON contains metadata and {len(papers)} papers")
        elif isinstance(data, list):
            papers = data
            print(f"ğŸ“Š Total papers loaded: {len(papers)}")
        else:
            print(f"âŒ Unexpected JSON structure: {type(data)}")
            return []
        
        # ç­›é€‰ç›¸å…³è®ºæ–‡ï¼ˆæ ‡è®°ä¸ºç›®æ ‡è®ºæ–‡çš„ï¼‰
        relevant_papers = [p for p in papers if p.get("is_target_paper", False)]
        print(f"ğŸ¯ Relevant papers: {len(relevant_papers)}")
        
        # åŠ è½½parquetæ–‡ä»¶è·å–abstract
        if parquet_file:
            print(f"ğŸ“– Loading abstracts from: {parquet_file}")
            df = pd.read_parquet(parquet_file)
            print(f"ğŸ“Š Parquet contains {len(df)} papers")
            
            # åˆ›å»ºarxiv_idåˆ°abstractçš„æ˜ å°„
            abstract_map = {}
            for _, row in df.iterrows():
                arxiv_id = row.get('arxiv_id', '')
                abstract = row.get('abstract', '')
                if arxiv_id and abstract:
                    abstract_map[arxiv_id] = abstract
            
            print(f"ğŸ“ Found abstracts for {len(abstract_map)} papers")
        else:
            abstract_map = {}
            print("âš ï¸  No parquet file provided, abstracts will be empty")
        
        # æŸ¥æ‰¾åŒ…å«"graph"çš„è®ºæ–‡
        graph_papers = []
        graph_pattern = re.compile(r'graph', re.IGNORECASE)
        
        for paper in relevant_papers:
            title = paper.get('title', '')
            arxiv_id = paper.get('arxiv_id', '')
            abstract = abstract_map.get(arxiv_id, '')
            
            # æ£€æŸ¥æ ‡é¢˜æˆ–æ‘˜è¦æ˜¯å¦åŒ…å«"graph"
            if graph_pattern.search(title) or graph_pattern.search(abstract):
                # å°†abstractæ·»åŠ åˆ°paperä¸­
                paper_with_abstract = paper.copy()
                paper_with_abstract['abstract'] = abstract
                graph_papers.append(paper_with_abstract)
        
        print(f"ğŸ“ˆ Papers containing 'graph': {len(graph_papers)}")
        
        # å‡†å¤‡è¾“å‡ºå†…å®¹
        output_lines = []
        
        for i, paper in enumerate(graph_papers, 1):
            title = paper.get('title', 'Unknown Title').strip()
            abstract = paper.get('abstract', 'No abstract available').strip()
            arxiv_id = paper.get('arxiv_id', 'N/A')
            score = paper.get('overall_score', 0)
            
            # æ ¼å¼åŒ–è¾“å‡º
            output_lines.append(f"## {title}")
            output_lines.append(f"{abstract}")
            output_lines.append(f"<!-- ArXiv ID: {arxiv_id}, Score: {score} -->")
            output_lines.append("")  # ç©ºè¡Œåˆ†éš”
            
            # æ§åˆ¶å°æ˜¾ç¤ºè¿›åº¦
            if i <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"\n{i}. {title[:100]}...")
        
        # è¾“å‡ºç»“æœ
        output_content = "\n".join(output_lines)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(output_content)
            print(f"\nğŸ’¾ Results saved to: {output_file}")
        else:
            print("\n" + "="*80)
            print("GRAPH PAPERS EXTRACTION RESULTS")
            print("="*80)
            print(output_content)
        
        return graph_papers
        
    except FileNotFoundError:
        print(f"âŒ File not found: {json_file}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ Invalid JSON format: {e}")
        return []
    except Exception as e:
        print(f"âŒ Error processing file: {e}")
        return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä»ç­›é€‰ç»“æœä¸­æå–åŒ…å«'graph'å…³é”®è¯çš„è®ºæ–‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä»æœ€æ–°çš„ç­›é€‰ç»“æœä¸­æå–ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾parquetæ–‡ä»¶ï¼‰
  python scripts/extract_graph_papers.py
  
  # æŒ‡å®šJSONå’Œparquetæ–‡ä»¶
  python scripts/extract_graph_papers.py --input arxiv_data/graph_ai_papers_20250813_144726.json --parquet arxiv_data/cs_papers_6months_20250812_002744.parquet
  
  # ä¿å­˜åˆ°æ–‡ä»¶
  python scripts/extract_graph_papers.py --output graph_papers_extracted.md
  
  # å®Œæ•´æŒ‡å®šæ‰€æœ‰å‚æ•°
  python scripts/extract_graph_papers.py --input arxiv_data/graph_ai_papers_20250813_144726.json --parquet arxiv_data/cs_papers_6months_20250812_002744.parquet --output results/graph_papers.md
        """
    )
    
    parser.add_argument("--input", "-i", 
                        help="è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨arxiv_dataç›®å½•ä¸‹æœ€æ–°çš„æ–‡ä»¶ï¼‰")
    parser.add_argument("--parquet", "-p", 
                        help="åŒ…å«abstractçš„parquetæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨arxiv_dataç›®å½•ä¸‹æœ€æ–°çš„æ–‡ä»¶ï¼‰")
    parser.add_argument("--output", "-o", 
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰")
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.input:
        input_file = args.input
    else:
        # æŸ¥æ‰¾arxiv_dataç›®å½•ä¸‹æœ€æ–°çš„graph_ai_papersæ–‡ä»¶
        arxiv_dir = "arxiv_data"
        if os.path.exists(arxiv_dir):
            json_files = [f for f in os.listdir(arxiv_dir) 
                         if f.startswith("graph_ai_papers_") and f.endswith(".json") 
                         and not f.endswith("_checkpoint.json")]
            
            if json_files:
                # æŒ‰æ–‡ä»¶åæ’åºï¼Œå–æœ€æ–°çš„
                latest_file = sorted(json_files)[-1]
                input_file = os.path.join(arxiv_dir, latest_file)
                print(f"ğŸ” Using latest file: {input_file}")
            else:
                print("âŒ No graph_ai_papers JSON files found in arxiv_data directory")
                return
        else:
            print("âŒ arxiv_data directory not found")
            return
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ Input file not found: {input_file}")
        return
    
    # ç¡®å®šparquetæ–‡ä»¶
    if args.parquet:
        parquet_file = args.parquet
    else:
        # æŸ¥æ‰¾arxiv_dataç›®å½•ä¸‹æœ€æ–°çš„cs_papers parquetæ–‡ä»¶
        arxiv_dir = "arxiv_data"
        if os.path.exists(arxiv_dir):
            parquet_files = [f for f in os.listdir(arxiv_dir) 
                           if f.startswith("cs_papers_") and f.endswith(".parquet")]
            
            if parquet_files:
                # æŒ‰æ–‡ä»¶åæ’åºï¼Œå–æœ€æ–°çš„
                latest_parquet = sorted(parquet_files)[-1]
                parquet_file = os.path.join(arxiv_dir, latest_parquet)
                print(f"ğŸ” Using latest parquet file: {parquet_file}")
            else:
                print("âš ï¸  No cs_papers parquet files found in arxiv_data directory")
                parquet_file = None
        else:
            parquet_file = None
    
    # æ£€æŸ¥parquetæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if parquet_file and not os.path.exists(parquet_file):
        print(f"âŒ Parquet file not found: {parquet_file}")
        parquet_file = None
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶
    output_file = args.output
    if output_file:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ğŸ“ Created output directory: {output_dir}")
    
    # æ‰§è¡Œæå–
    graph_papers = extract_graph_papers(input_file, parquet_file, output_file)
    
    if graph_papers:
        print(f"\nâœ… Successfully extracted {len(graph_papers)} papers containing 'graph'")
        if not output_file:
            print("\nğŸ’¡ Use --output to save results to a file")
    else:
        print("\nâŒ No papers found or extraction failed")


if __name__ == "__main__":
    main()